{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries and Dependencies\n",
        "\n"
      ],
      "metadata": {
        "id": "SrF7kZFOmqD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiQLT6bdELiO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%env TORCH=2.1.0+cu121\n",
        "!pip install torch_scatter torch_sparse torch_spline_conv -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install torchmetrics ogb rdkit\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.nn import Module, Dropout, LayerNorm, Identity\n",
        "from torch_geometric.data import DataLoader, Data, InMemoryDataset\n",
        "from torch_geometric.nn import GINConv, GINEConv, global_add_pool\n",
        "from torch_geometric.typing import Adj, OptTensor\n",
        "from torch_geometric.utils import degree\n",
        "from torch_geometric.utils.convert import from_networkx\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "import networkx as nx\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch_geometric.nn as gnn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "YoAPsGyamzjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044a7c6b-54ab-4f8a-facc-987091c1fb6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:99: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "NszYZ7ltoYHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "# Save the object to a file\n",
        "file_path = 'transformed_dataset_20.pkl'  # Specify the path and file name\n",
        "\n",
        "# Opening the file in binary-write mode and saving with pickle\n",
        "with open(file_path, 'rb') as file:\n",
        "    transformed_dataset_20 = pickle.load( file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "e5L0w_hgY7zO",
        "outputId": "00f35a52-3c2b-45c3-f298-e0cb52b8837a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:99: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "Ran out of input",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-27d7a9c5aba7>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Opening the file in binary-write mode and saving with pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtransformed_dataset_20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASETS"
      ],
      "metadata": {
        "id": "ux4C3Xx-eiOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data, slices = torch.load('exp_dataset.pt')\n",
        "\n",
        "class PlanarSATPairsDataset(InMemoryDataset):\n",
        "    def __init__(self):\n",
        "        super(PlanarSATPairsDataset, self).__init__()\n",
        "        self.data, self.slices = data, slices\n",
        "exp_dataset = PlanarSATPairsDataset()"
      ],
      "metadata": {
        "id": "KVkRkrYOelOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "1Q2d7HLXe0Gy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(exp_dataset)"
      ],
      "metadata": {
        "id": "1uHrQ-iy06FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(dataset):\n",
        "    # Extract data and slices from the dataset\n",
        "    data, slices = dataset.data, dataset.slices\n",
        "\n",
        "    # Prepare lists to hold separate graph Data objects, features, and labels\n",
        "    graphs = []\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    # Iterating over the range of total graphs (assuming 'y' is always present and is per graph)\n",
        "    for i in range(len(slices['y']) - 1):\n",
        "        # Extract indices for node features and edge indices\n",
        "        x_start, x_end = slices['x'][i], slices['x'][i+1]\n",
        "        edge_index_start, edge_index_end = slices['edge_index'][i], slices['edge_index'][i+1]\n",
        "        y = data.y[slices['y'][i]:slices['y'][i+1]]  # y is typically a single label if per graph\n",
        "\n",
        "        # Create Data object for each graph\n",
        "        graph = Data(x=data.x[x_start:x_end], edge_index=data.edge_index[:, edge_index_start:edge_index_end], y=y)\n",
        "        graphs.append(graph)\n",
        "\n",
        "\n",
        "    return graphs"
      ],
      "metadata": {
        "id": "ThJzXQwye1J1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_enclosing_subgraph(graph, node_pair, h):\n",
        "    \"\"\"Extracts the h-hop enclosing subgraph around a node pair and adds an edge between them.\"\"\"\n",
        "    u, v = node_pair\n",
        "    # Get nodes reachable within h hops from both u and v\n",
        "    reachable_from_u = set(nx.single_source_shortest_path_length(graph, u, cutoff=h).keys())\n",
        "    reachable_from_v = set(nx.single_source_shortest_path_length(graph, v, cutoff=h).keys())\n",
        "    enclosing_nodes = reachable_from_u.union(reachable_from_v)\n",
        "\n",
        "    # Create subgraph and ensure it includes the edge between u and v\n",
        "    subgraph = graph.subgraph(enclosing_nodes).copy()\n",
        "    subgraph.add_edge(u, v)  # Add the edge, which might not exist in the original graph\n",
        "\n",
        "    return subgraph, (u,v)\n",
        "\n",
        "def create_line_graph_and_features(subgraph, original_features, node_pair):\n",
        "    \"\"\"\n",
        "    Creates a line graph from a subgraph with new node features being the sum of the\n",
        "    original edge nodes' features and finds the new index of a node in the line graph\n",
        "    corresponding to the node_pair.\n",
        "    \"\"\"\n",
        "    # Create the line graph from the subgraph\n",
        "    line_graph = nx.line_graph(subgraph)\n",
        "\n",
        "    # Compute features for the line graph nodes\n",
        "    line_features = {}\n",
        "    for edge in line_graph.nodes():\n",
        "        # Sum features of the original nodes that form the edge in the subgraph\n",
        "        line_features[edge] = original_features[edge[0]] + original_features[edge[1]]\n",
        "\n",
        "    # Convert the line graph to a PyG Data object\n",
        "    lg_data = from_networkx(line_graph)\n",
        "\n",
        "    # Assign features to the line graph in PyG\n",
        "    lg_features = torch.stack([line_features[edge] for edge in line_graph.nodes()])\n",
        "    lg_data.x = lg_features\n",
        "\n",
        "    # Determine the node in the line graph corresponding to the node_pair\n",
        "    # It's assumed the node_pair is an edge in the original subgraph\n",
        "    node_mapping = {edge: i for i, edge in enumerate(line_graph.nodes())}\n",
        "    node_pair = tuple(sorted(node_pair))  # Ensure the pair is sorted (smaller, larger) if undirected\n",
        "    new_index = node_mapping.get(node_pair, -1)  # -1 if not found\n",
        "\n",
        "    return lg_data, new_index"
      ],
      "metadata": {
        "id": "dfVaOOFSEIs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import networkx as nx\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import to_networkx, from_networkx\n",
        "from itertools import combinations\n",
        "\n",
        "def transform_graph_to_subgraphs(graph_list, h):\n",
        "    transformed_data = []\n",
        "\n",
        "    for data in graph_list:\n",
        "        G = to_networkx(data, to_undirected=True)\n",
        "        num_nodes = data.num_nodes\n",
        "        original_features = {node: data.x[i] for i, node in enumerate(G.nodes())}\n",
        "\n",
        "        line_graphs = []\n",
        "\n",
        "        y = data.y\n",
        "\n",
        "        # Iterate over all pairs of nodes\n",
        "        for node_pair in combinations(range(num_nodes), 2):\n",
        "            # Extract the h-hop enclosing subgraph around these two nodes and the edge\n",
        "            subgraph, _ = extract_enclosing_subgraph(G, node_pair, h)\n",
        "            lg, new_index = create_line_graph_and_features(subgraph, original_features, node_pair)\n",
        "\n",
        "            line_graphs.append((lg, new_index))\n",
        "\n",
        "\n",
        "        # Save the tuple for this graph\n",
        "        transformed_data.append((line_graphs, y))\n",
        "\n",
        "    return transformed_data\n",
        "\n"
      ],
      "metadata": {
        "id": "EPjZG3c_5_n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graphs_list = split_dataset(exp_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYW5IjYzXy-R",
        "outputId": "e2a3dace-c068-4448-8d7f-48f0767d059f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/data/in_memory_dataset.py:300: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "transformed_exp_datasets = []\n",
        "for h in [2, 3, 5, 20, 30]:\n",
        "  start_time = time.time()  # Record the start time\n",
        "\n",
        "  transformed_exp_datasets.append(transform_graph_to_subgraphs(graphs_list, h))\n",
        "\n",
        "  elapsed_time = time.time() - start_time  # Calculate the elapsed time\n",
        "  print(\"Time taken to transform dataset {h}: {:.2f} seconds\".format(h, elapsed_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ht5lB2xcXnNl",
        "outputId": "665dc1de-aade-4616-b8f0-c8b9693ea6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to transform dataset: 4467.49 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_dataset_5 = transformed_exp_datasets[2]"
      ],
      "metadata": {
        "id": "ZzOdyU86mKtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the object to a file\n",
        "file_path = 'transformed_dataset_5.pkl'  # Specify the path and file name\n",
        "\n",
        "# Opening the file in binary-write mode and saving with pickle\n",
        "with open(file_path, 'wb') as file:\n",
        "    pickle.dump(transformed_dataset_5, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "glFrjFEZhnaF",
        "outputId": "b59c4ba7-4b48-4755-938b-6bbd58de7d63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'transformed_dataset_5' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f83f655e8e44>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Opening the file in binary-write mode and saving with pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_dataset_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'transformed_dataset_5' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The range of different graph sizes\n",
        "size_range = range(6, 16)\n",
        "\n",
        "# The list of all graph pairs\n",
        "graph_pairs = []\n",
        "\n",
        "for n in size_range:\n",
        "  for i in range(2, n//2 + 1):\n",
        "    C_n = nx.cycle_graph(n)\n",
        "    D_ni = nx.disjoint_union(nx.cycle_graph(n-i), nx.cycle_graph(i))\n",
        "    graph_pairs.append((C_n, D_ni))\n",
        "# The list of Data objects\n",
        "dataset_cicles = []\n",
        "\n",
        "# Loop over graph pairs\n",
        "for pair in graph_pairs:\n",
        "    # Treat each element of the pair\n",
        "    for i, graph in enumerate(pair):\n",
        "      graph_geom = from_networkx(graph)\n",
        "\n",
        "      # Set the input feature x (same for all nodes)\n",
        "      graph_geom.x = torch.ones((graph.number_of_nodes(), 50))\n",
        "\n",
        "      # Set the output feature y (1 for cycle, 0 for disjoint)\n",
        "      graph_geom.y = torch.tensor([1 if i == 0 else 0])\n",
        "\n",
        "      # Add the Data object to the dataset\n",
        "      dataset_cicles.append(graph_geom)\n",
        "\n",
        "print(dataset[0], dataset[0].y)\n",
        "print(dataset[0], dataset[1].y)\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "id": "ii_-yo1RJ4G1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNInit(nn.Module):\n",
        "\n",
        "    def __init__(self, random_dims = 1, distribution = 'n'):\n",
        "        super().__init__()\n",
        "        self.random_dims = random_dims\n",
        "        self.distribution = distribution\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.random_dims > 0:\n",
        "\n",
        "            rand = torch.empty(( x.shape[0], self.random_dims)).to(device)\n",
        "            if self.distribution == \"n\":\n",
        "                torch.nn.init.normal_(rand)\n",
        "            elif self.distribution == \"u\":\n",
        "                torch.nn.init.uniform_(rand, a=-1.0, b=1.0)\n",
        "            elif self.distribution == \"xn\":\n",
        "                torch.nn.init.xavier_normal_(rand)\n",
        "            elif self.distribution == \"xu\":\n",
        "                torch.nn.init.xavier_uniform_(rand)\n",
        "            elif self.distribution == 'ones':\n",
        "               torch.nn.init.ones_(rand)\n",
        "            x = torch.cat([x, rand], dim=1).to(device)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "v5q_7_zFYHWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LGNN"
      ],
      "metadata": {
        "id": "o8rthrQ5nMPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GraphConv, global_add_pool\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "class LGNN(nn.Module):\n",
        "    def __init__(self, num_layers=7, input_dim=2, hidden_dim=8, output_dim=2, rni = RNInit(), r = True):\n",
        "        super(LGNN, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        self.layers.append(GraphConv(input_dim, hidden_dim, aggr='add'))  # Using add aggregation\n",
        "        for _ in range(num_layers - 2):  # minus 2 because first and last layers are separately defined\n",
        "            self.layers.append(GraphConv(hidden_dim, hidden_dim, aggr='add'))\n",
        "        self.layers.append(GraphConv(hidden_dim, hidden_dim, aggr='add'))\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ELU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_dim, 32),\n",
        "            nn.ELU(),\n",
        "            nn.Linear(32, output_dim)\n",
        "        )\n",
        "        self.activation = nn.ELU()\n",
        "        self.r = r\n",
        "        self.rni = rni\n",
        "\n",
        "    def forward(self, data):\n",
        "        embedding_LG = []\n",
        "        for line_graph, node_idx in data:\n",
        "            line_graph = line_graph.to(device)\n",
        "\n",
        "            x, edge_index = line_graph.x, line_graph.edge_index\n",
        "            if self.r:\n",
        "              x = self.rni(x)\n",
        "            for layer in self.layers:\n",
        "                x = self.activation(layer(x, edge_index))\n",
        "\n",
        "            # Extract the embedding of the specified node\n",
        "            embedding_LG.append(x[node_idx])\n",
        "\n",
        "        # Pooling: Sum the features across the embeddings\n",
        "        logit = self.pooling(embedding_LG)\n",
        "        out = self.mlp(logit)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def pooling(self, embeddings):\n",
        "        # Sum the embeddings along the 0th dimension\n",
        "        return torch.sum(torch.stack(embeddings), dim=0)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.layers:\n",
        "            layer.reset_parameters()"
      ],
      "metadata": {
        "id": "gqCwOw7JehJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING"
      ],
      "metadata": {
        "id": "ZnNd03Cke8QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.loader import DataLoader  # Correct import\n",
        "import torch\n",
        "\n",
        "# Define the dataset in a way that aligns with how it is structured\n",
        "# This example assumes transformed_dataset_2 is already properly defined as per your description\n",
        "\n",
        "# Simplify the train function to focus on handling this complex structure\n",
        "\n",
        "def train(model, loader, optimizer, device, rni = RNInit()):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_items = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        # Assuming each 'batch' is a list of tuples (graph, label)\n",
        "        batch_graphs, batch_labels = zip(*batch)\n",
        "        outputs = []\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for graph, label in zip(batch_graphs, batch_labels):\n",
        "           # Assuming `graph` is already a tensor or compatible type\n",
        "            out = model(graph)\n",
        "            outputs.append(out.unsqueeze(0))  # Unsqueeze to simulate batch dimension for stacking later\n",
        "\n",
        "        outputs = torch.cat(outputs, dim=0)  # Concatenate along a new dimension to create a batch tensor for loss calculation\n",
        "        outputs = torch.log_softmax(outputs, dim=1)\n",
        "        labels = torch.tensor(batch_labels, device=device)  # Convert labels to tensor and send to device\n",
        "\n",
        "        loss = F.nll_loss(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        total_items += len(batch)\n",
        "\n",
        "    return total_loss / total_items  # Average loss per item\n",
        "\n",
        "def val(model, loader, device, rni = RNInit()):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_items = 0\n",
        "    with torch.no_grad():\n",
        "        outputs = []\n",
        "        labels = []\n",
        "\n",
        "        for batch in loader:\n",
        "            for data, y in batch:\n",
        "                data = data # Make sure data is on the correct device\n",
        "                output = model(data)  # Process each item\n",
        "                outputs.append(output.unsqueeze(0))  # Unsqueeze to simulate batch dimension\n",
        "                labels.append(y)\n",
        "\n",
        "        # Concatenate all outputs and labels into tensors\n",
        "        outputs = torch.cat(outputs, dim=0)\n",
        "        labels = torch.tensor(labels, device=device)\n",
        "\n",
        "        # Apply log_softmax to the concatenated outputs\n",
        "        outputs = torch.log_softmax(outputs, dim=1)\n",
        "\n",
        "        # Calculate total loss using all outputs and labels\n",
        "        loss = F.nll_loss(outputs, labels, reduction='sum')\n",
        "        total_loss = loss.item()\n",
        "        total_items = len(labels)\n",
        "\n",
        "    return total_loss / total_items  # Return the average loss per item\n",
        "\n",
        "\n",
        "def test(model, loader, device, rni = RNInit()):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            predictions = []\n",
        "            for data, y in batch:\n",
        "                output = model(data)\n",
        "                pred = output.argmax(-1)\n",
        "                predictions.append(pred)\n",
        "            labels = torch.tensor([y for _, y in batch]).to(device)\n",
        "            p = torch.tensor(predictions).to(device)\n",
        "            correct += (p == labels).sum().item()\n",
        "    return correct / len(loader.dataset)"
      ],
      "metadata": {
        "id": "U3H4KB8XfBhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class ManualDataLoader:\n",
        "    def __init__(self, dataset, batch_size, shuffle=False):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.idx = 0\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self.dataset)\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.idx >= len(self.dataset):\n",
        "            raise StopIteration\n",
        "        batch = self.dataset[self.idx:self.idx+self.batch_size]\n",
        "        self.idx += self.batch_size\n",
        "        return batch\n"
      ],
      "metadata": {
        "id": "-XkD2c_dxugl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(test_dataset[0][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMtiqIjVwOFi",
        "outputId": "518199c6-7aff-48d2-c946-4d3b8b6517b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataset, epochs=300, learning_rate=0.001, batch_size=20, splits=10,  modulo=4, mod_thresh=1):\n",
        "    tr_accuracies = np.zeros((epochs, splits))\n",
        "    tst_accuracies = np.zeros((epochs, splits))\n",
        "\n",
        "    n = len(dataset) // splits\n",
        "\n",
        "    for i in range(splits):\n",
        "        model.reset_parameters()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=5, min_lr=learning_rate)\n",
        "\n",
        "        test_mask = torch.zeros(len(dataset), dtype=torch.bool)\n",
        "        test_mask[i * n:(i + 1) * n] = 1\n",
        "        learning_indices = torch.where((torch.arange(len(dataset)) % modulo <= mod_thresh) & ~test_mask)[0]\n",
        "        exp_indices = torch.where((torch.arange(len(dataset)) % modulo > mod_thresh) & ~test_mask)[0]\n",
        "\n",
        "        # Convert boolean masks to lists of indices\n",
        "        test_indices = test_mask.nonzero(as_tuple=False).view(-1).tolist()\n",
        "        learning_indices = learning_indices.tolist()\n",
        "        exp_indices = exp_indices.tolist()\n",
        "\n",
        "        # Now load the datasets using lists of indices\n",
        "        test_dataset = [dataset[idx] for idx in test_indices]\n",
        "        test_exp_dataset = [dataset[idx] for idx in exp_indices]\n",
        "        test_lrn_dataset = [dataset[idx] for idx in learning_indices]\n",
        "        train_dataset = [dataset[idx] for idx in range(len(dataset)) if idx not in test_indices]\n",
        "\n",
        "        # Creating data loaders\n",
        "        val_loader = ManualDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        test_loader = ManualDataLoader(test_dataset, batch_size=batch_size)\n",
        "        test_exp_loader = ManualDataLoader(test_exp_dataset, batch_size=batch_size)\n",
        "        test_lrn_loader = ManualDataLoader(test_lrn_dataset, batch_size=batch_size)\n",
        "        train_loader = ManualDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "        print('---------------- Split {} ----------------'.format(i))\n",
        "        best_val_loss, best_test_acc = float('inf'), 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_loss = train(model, train_loader, optimizer, device)  # Assuming train is a predefined function\n",
        "            val_loss = val(model, train_loader, device)  # Using train loader for validation to simplify\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_test_acc = test(model, test_loader, device)\n",
        "\n",
        "            tr_accuracies[epoch, i] = test(model, train_loader, device)\n",
        "            tst_accuracies[epoch, i] = best_test_acc\n",
        "\n",
        "            print(f'Epoch: {epoch+1}, LR: {scheduler.optimizer.param_groups[0][\"lr\"]:.6f}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Acc: {best_test_acc:.4f}')\n",
        "\n",
        "    return tr_accuracies, tst_accuracies"
      ],
      "metadata": {
        "id": "MgBAkuWWfKGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LGNN(input_dim=2, num_layers=3, hidden_dim=64, output_dim=2).to(device)"
      ],
      "metadata": {
        "id": "9VnLGmC1lDf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset for training with associated tuple groups maintained\n",
        "results = train_model(model, dataset=transformed_dataset_20, epochs=10, learning_rate=0.001, batch_size=5, splits=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJpiqVk4lou-",
        "outputId": "9ea4e33e-c432-46de-effe-eb29da36713e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- Split 0 ----------------\n",
            "Epoch: 1, LR: 0.001000, Train Loss: 36.4952, Val Loss: 10.6937, Test Acc: 0.5000\n",
            "Epoch: 2, LR: 0.001000, Train Loss: 13.2766, Val Loss: 27.6723, Test Acc: 0.5000\n",
            "---------------- Split 1 ----------------\n",
            "Epoch: 1, LR: 0.001000, Train Loss: 6.6842, Val Loss: 3.7899, Test Acc: 0.5000\n",
            "Epoch: 2, LR: 0.001000, Train Loss: 2.2765, Val Loss: 9.9686, Test Acc: 0.5000\n",
            "---------------- Split 2 ----------------\n",
            "Epoch: 1, LR: 0.001000, Train Loss: 47.0610, Val Loss: 187.5068, Test Acc: 0.5000\n",
            "Epoch: 2, LR: 0.001000, Train Loss: 75.0756, Val Loss: 95.0152, Test Acc: 0.5000\n",
            "---------------- Split 3 ----------------\n",
            "Epoch: 1, LR: 0.001000, Train Loss: 69.6269, Val Loss: 86.5442, Test Acc: 0.5000\n",
            "Epoch: 2, LR: 0.001000, Train Loss: 14.8983, Val Loss: 28.4975, Test Acc: 0.5000\n",
            "---------------- Split 4 ----------------\n",
            "Epoch: 1, LR: 0.001000, Train Loss: 18.8354, Val Loss: 52.5636, Test Acc: 0.5000\n",
            "Epoch: 2, LR: 0.001000, Train Loss: 13.4391, Val Loss: 68.6941, Test Acc: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = LGNN(input_dim=3, num_layers=3, hidden_dim=64, output_dim=2).to(device)"
      ],
      "metadata": {
        "id": "4a4Ppdm1YVay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "td_s = transformed_dataset_5[:20] + transformed_dataset_5[600:620]"
      ],
      "metadata": {
        "id": "-MqDflzAmmnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_model(model2, dataset=td_s, epochs=30, learning_rate=0.001, batch_size=10, splits=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "id": "D-1fjISaYaUT",
        "outputId": "97c71f55-75d5-43c5-d257-80efcd853396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- Split 0 ----------------\n",
            "Epoch: 1, LR: 0.001000, Train Loss: 23.7473, Val Loss: 20.5804, Test Acc: 0.5000\n",
            "Epoch: 2, LR: 0.001000, Train Loss: 9.6539, Val Loss: 19.0260, Test Acc: 0.5000\n",
            "Epoch: 3, LR: 0.001000, Train Loss: 6.6140, Val Loss: 14.2363, Test Acc: 0.5000\n",
            "Epoch: 4, LR: 0.001000, Train Loss: 5.3140, Val Loss: 3.1371, Test Acc: 0.5000\n",
            "Epoch: 5, LR: 0.001000, Train Loss: 8.7168, Val Loss: 54.3773, Test Acc: 0.5000\n",
            "Epoch: 6, LR: 0.001000, Train Loss: 5.2164, Val Loss: 25.4123, Test Acc: 0.5000\n",
            "Epoch: 7, LR: 0.001000, Train Loss: 11.7806, Val Loss: 7.4605, Test Acc: 0.5000\n",
            "Epoch: 8, LR: 0.001000, Train Loss: 5.4680, Val Loss: 40.0775, Test Acc: 0.5000\n",
            "Epoch: 9, LR: 0.001000, Train Loss: 5.6977, Val Loss: 19.9184, Test Acc: 0.5000\n",
            "Epoch: 10, LR: 0.001000, Train Loss: 7.9545, Val Loss: 8.2928, Test Acc: 0.5000\n",
            "Epoch: 11, LR: 0.001000, Train Loss: 5.5633, Val Loss: 11.6710, Test Acc: 0.5000\n",
            "Epoch: 12, LR: 0.001000, Train Loss: 3.9070, Val Loss: 5.3288, Test Acc: 0.5000\n",
            "Epoch: 13, LR: 0.001000, Train Loss: 1.8763, Val Loss: 6.3311, Test Acc: 0.5000\n",
            "Epoch: 14, LR: 0.001000, Train Loss: 2.4314, Val Loss: 7.8243, Test Acc: 0.5000\n",
            "Epoch: 15, LR: 0.001000, Train Loss: 1.7289, Val Loss: 20.0786, Test Acc: 0.5000\n",
            "Epoch: 16, LR: 0.001000, Train Loss: 2.3027, Val Loss: 13.0320, Test Acc: 0.5000\n",
            "Epoch: 17, LR: 0.001000, Train Loss: 2.2124, Val Loss: 15.1718, Test Acc: 0.5000\n",
            "Epoch: 18, LR: 0.001000, Train Loss: 1.8300, Val Loss: 16.2131, Test Acc: 0.5000\n",
            "Epoch: 19, LR: 0.001000, Train Loss: 0.7505, Val Loss: 14.2259, Test Acc: 0.5000\n",
            "Epoch: 20, LR: 0.001000, Train Loss: 0.7657, Val Loss: 2.8857, Test Acc: 0.5000\n",
            "Epoch: 21, LR: 0.001000, Train Loss: 0.4984, Val Loss: 4.2086, Test Acc: 0.5000\n",
            "Epoch: 22, LR: 0.001000, Train Loss: 0.8632, Val Loss: 8.5975, Test Acc: 0.5000\n",
            "Epoch: 23, LR: 0.001000, Train Loss: 0.8763, Val Loss: 9.0756, Test Acc: 0.5000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-447930c566f5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtd_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-d759d7f41d3f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataset, epochs, learning_rate, batch_size, splits, modulo, mod_thresh)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming train is a predefined function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Using train loader for validation to simplify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-b4526aa8afad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, device, rni)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_graphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m            \u001b[0;31m# Assuming `graph` is already a tensor or compatible type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Unsqueeze to simulate batch dimension for stacking later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-b065e0d02844>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m               \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrni\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0f858169f815>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_dims\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mrand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"n\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = LGNN(input_dim=22, num_layers=5, hidden_dim=64, output_dim=2, rni = RNInit(random_dims = 20)).to(device)"
      ],
      "metadata": {
        "id": "JkwnCXqh1P5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = train_model(model3, dataset=td_s, epochs=30, learning_rate=0.001, batch_size=10, splits=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qI4xheD1rAg",
        "outputId": "a56f7ceb-0355-4d5f-a85e-4726b09df05f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- Split 0 ----------------\n",
            "Epoch: 1, LR: 0.001000, Train Loss: 16.3518, Val Loss: 15.8291, Test Acc: 0.5385\n",
            "Epoch: 2, LR: 0.001000, Train Loss: 43.5235, Val Loss: 71.4959, Test Acc: 0.5385\n",
            "Epoch: 3, LR: 0.001000, Train Loss: 19.7233, Val Loss: 80.8621, Test Acc: 0.5385\n",
            "Epoch: 4, LR: 0.001000, Train Loss: 26.5086, Val Loss: 93.1845, Test Acc: 0.5385\n",
            "Epoch: 5, LR: 0.001000, Train Loss: 10.3807, Val Loss: 22.1482, Test Acc: 0.5385\n",
            "Epoch: 6, LR: 0.001000, Train Loss: 12.8552, Val Loss: 36.8608, Test Acc: 0.5385\n",
            "Epoch: 7, LR: 0.001000, Train Loss: 5.1088, Val Loss: 45.6782, Test Acc: 0.5385\n",
            "Epoch: 8, LR: 0.001000, Train Loss: 5.4612, Val Loss: 27.5692, Test Acc: 0.5385\n",
            "Epoch: 9, LR: 0.001000, Train Loss: 7.1929, Val Loss: 11.1359, Test Acc: 0.5385\n",
            "Epoch: 10, LR: 0.001000, Train Loss: 5.9018, Val Loss: 11.1654, Test Acc: 0.5385\n",
            "Epoch: 11, LR: 0.001000, Train Loss: 4.4824, Val Loss: 25.2463, Test Acc: 0.5385\n",
            "Epoch: 12, LR: 0.001000, Train Loss: 4.8207, Val Loss: 29.8293, Test Acc: 0.5385\n",
            "Epoch: 13, LR: 0.001000, Train Loss: 4.1139, Val Loss: 37.2316, Test Acc: 0.5385\n",
            "Epoch: 14, LR: 0.001000, Train Loss: 5.3603, Val Loss: 70.7262, Test Acc: 0.5385\n",
            "Epoch: 15, LR: 0.001000, Train Loss: 8.0207, Val Loss: 44.0587, Test Acc: 0.5385\n",
            "Epoch: 16, LR: 0.001000, Train Loss: 3.2811, Val Loss: 3.7586, Test Acc: 0.5385\n",
            "Epoch: 17, LR: 0.001000, Train Loss: 5.4245, Val Loss: 16.0956, Test Acc: 0.5385\n",
            "Epoch: 18, LR: 0.001000, Train Loss: 4.5177, Val Loss: 17.2170, Test Acc: 0.5385\n",
            "Epoch: 19, LR: 0.001000, Train Loss: 3.4114, Val Loss: 7.1644, Test Acc: 0.5385\n",
            "Epoch: 20, LR: 0.001000, Train Loss: 1.7524, Val Loss: 7.1857, Test Acc: 0.5385\n",
            "Epoch: 21, LR: 0.001000, Train Loss: 1.0329, Val Loss: 5.3378, Test Acc: 0.5385\n",
            "Epoch: 22, LR: 0.001000, Train Loss: 2.5801, Val Loss: 9.8988, Test Acc: 0.5385\n",
            "Epoch: 23, LR: 0.001000, Train Loss: 1.7927, Val Loss: 7.2674, Test Acc: 0.5385\n",
            "Epoch: 24, LR: 0.001000, Train Loss: 1.9893, Val Loss: 4.5091, Test Acc: 0.5385\n",
            "Epoch: 25, LR: 0.001000, Train Loss: 1.5250, Val Loss: 2.7673, Test Acc: 0.4615\n",
            "Epoch: 26, LR: 0.001000, Train Loss: 0.9923, Val Loss: 3.6195, Test Acc: 0.4615\n",
            "Epoch: 27, LR: 0.001000, Train Loss: 0.6798, Val Loss: 7.5737, Test Acc: 0.4615\n",
            "Epoch: 28, LR: 0.001000, Train Loss: 1.3842, Val Loss: 11.4660, Test Acc: 0.4615\n",
            "Epoch: 29, LR: 0.001000, Train Loss: 1.1956, Val Loss: 8.5843, Test Acc: 0.4615\n",
            "Epoch: 30, LR: 0.001000, Train Loss: 0.9869, Val Loss: 11.0520, Test Acc: 0.4615\n",
            "---------------- Split 1 ----------------\n",
            "Epoch: 1, LR: 0.001000, Train Loss: 44.2548, Val Loss: 218.7747, Test Acc: 0.5385\n",
            "Epoch: 2, LR: 0.001000, Train Loss: 32.6630, Val Loss: 180.3567, Test Acc: 0.4615\n",
            "Epoch: 3, LR: 0.001000, Train Loss: 21.5861, Val Loss: 68.1749, Test Acc: 0.4615\n",
            "Epoch: 4, LR: 0.001000, Train Loss: 11.9832, Val Loss: 8.2924, Test Acc: 0.5385\n",
            "Epoch: 5, LR: 0.001000, Train Loss: 7.2991, Val Loss: 13.4609, Test Acc: 0.5385\n",
            "Epoch: 6, LR: 0.001000, Train Loss: 7.8356, Val Loss: 7.5497, Test Acc: 0.5385\n",
            "Epoch: 7, LR: 0.001000, Train Loss: 5.0725, Val Loss: 11.4230, Test Acc: 0.5385\n",
            "Epoch: 8, LR: 0.001000, Train Loss: 3.1911, Val Loss: 8.0922, Test Acc: 0.5385\n",
            "Epoch: 9, LR: 0.001000, Train Loss: 4.2913, Val Loss: 23.2833, Test Acc: 0.5385\n",
            "Epoch: 10, LR: 0.001000, Train Loss: 6.0709, Val Loss: 6.5152, Test Acc: 0.4615\n",
            "Epoch: 11, LR: 0.001000, Train Loss: 2.3601, Val Loss: 3.7144, Test Acc: 0.4615\n",
            "Epoch: 12, LR: 0.001000, Train Loss: 2.5328, Val Loss: 9.9651, Test Acc: 0.4615\n",
            "Epoch: 13, LR: 0.001000, Train Loss: 2.7324, Val Loss: 5.2530, Test Acc: 0.4615\n",
            "Epoch: 14, LR: 0.001000, Train Loss: 1.6441, Val Loss: 12.6327, Test Acc: 0.4615\n",
            "Epoch: 15, LR: 0.001000, Train Loss: 1.6611, Val Loss: 1.5681, Test Acc: 0.4615\n",
            "Epoch: 16, LR: 0.001000, Train Loss: 0.9405, Val Loss: 1.2921, Test Acc: 0.4615\n",
            "Epoch: 17, LR: 0.001000, Train Loss: 1.5488, Val Loss: 1.5956, Test Acc: 0.4615\n",
            "Epoch: 18, LR: 0.001000, Train Loss: 1.6060, Val Loss: 7.2357, Test Acc: 0.4615\n",
            "Epoch: 19, LR: 0.001000, Train Loss: 0.7674, Val Loss: 8.1248, Test Acc: 0.4615\n",
            "Epoch: 20, LR: 0.001000, Train Loss: 1.0650, Val Loss: 4.4288, Test Acc: 0.4615\n",
            "Epoch: 21, LR: 0.001000, Train Loss: 0.4031, Val Loss: 2.7139, Test Acc: 0.4615\n",
            "Epoch: 22, LR: 0.001000, Train Loss: 0.4780, Val Loss: 1.2905, Test Acc: 0.3846\n",
            "Epoch: 23, LR: 0.001000, Train Loss: 0.4759, Val Loss: 0.9820, Test Acc: 0.5385\n",
            "Epoch: 24, LR: 0.001000, Train Loss: 0.4614, Val Loss: 1.6205, Test Acc: 0.5385\n",
            "Epoch: 25, LR: 0.001000, Train Loss: 0.1650, Val Loss: 0.9920, Test Acc: 0.5385\n",
            "Epoch: 26, LR: 0.001000, Train Loss: 0.2722, Val Loss: 1.1462, Test Acc: 0.5385\n",
            "Epoch: 27, LR: 0.001000, Train Loss: 0.6564, Val Loss: 1.4681, Test Acc: 0.5385\n",
            "Epoch: 28, LR: 0.001000, Train Loss: 0.4549, Val Loss: 1.3838, Test Acc: 0.5385\n",
            "Epoch: 29, LR: 0.001000, Train Loss: 0.2809, Val Loss: 2.0013, Test Acc: 0.5385\n",
            "Epoch: 30, LR: 0.001000, Train Loss: 0.3862, Val Loss: 2.2070, Test Acc: 0.5385\n",
            "---------------- Split 2 ----------------\n",
            "Epoch: 1, LR: 0.001000, Train Loss: 45.6558, Val Loss: 84.5204, Test Acc: 0.5385\n",
            "Epoch: 2, LR: 0.001000, Train Loss: 23.0394, Val Loss: 4.7413, Test Acc: 0.6154\n",
            "Epoch: 3, LR: 0.001000, Train Loss: 8.2385, Val Loss: 41.8379, Test Acc: 0.6154\n",
            "Epoch: 4, LR: 0.001000, Train Loss: 13.4177, Val Loss: 18.4895, Test Acc: 0.6154\n",
            "Epoch: 5, LR: 0.001000, Train Loss: 7.2175, Val Loss: 5.8464, Test Acc: 0.6154\n",
            "Epoch: 6, LR: 0.001000, Train Loss: 7.8353, Val Loss: 22.5630, Test Acc: 0.6154\n",
            "Epoch: 7, LR: 0.001000, Train Loss: 3.4590, Val Loss: 11.0323, Test Acc: 0.6154\n",
            "Epoch: 8, LR: 0.001000, Train Loss: 5.7449, Val Loss: 18.5474, Test Acc: 0.6154\n",
            "Epoch: 9, LR: 0.001000, Train Loss: 2.2885, Val Loss: 46.8804, Test Acc: 0.6154\n",
            "Epoch: 10, LR: 0.001000, Train Loss: 4.5018, Val Loss: 19.0187, Test Acc: 0.6154\n",
            "Epoch: 11, LR: 0.001000, Train Loss: 8.1824, Val Loss: 16.8399, Test Acc: 0.6154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do not include the rest of the experiments conducted as the results are equivalent. We leave for future work how Random Node Initialisation affects the expressivity of LGNN in the context of discerning pair of graphs in the EXP dataset.\n"
      ],
      "metadata": {
        "id": "kkXr0-DhKroq"
      }
    }
  ]
}